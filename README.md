# ğŸ“˜ Book Search, Summarization & Evaluation (RAG System)

## ğŸ§© Problem Definition

In many real-world scenarios, users need to quickly search through large volumes of documents (books, reports, PDFs, notes) and obtain concise, meaningful summaries of relevant content. Traditional keyword-based search fails to capture semantic meaning and does not provide synthesized answers.

This project solves the problem by building a **Retrieval-Augmented Generation (RAG)** system that:
- Semantically indexes documents
- Retrieves the most relevant chunks for a user query
- Generates a concise summary using an LLM
- Evaluates summary quality using ROUGE metrics

The system is designed to be **persistent, scalable, and production-aligned**, where indexed documents remain searchable across sessions.

---

## ğŸ¯ Objectives

- Enable semantic search over uploaded documents
- Avoid duplicate indexing using content hashing
- Generate query-focused summaries
- Evaluate summaries using automatic and human evaluation
- Build an industry-grade RAG pipeline with clean modular design

---

## ğŸ§ª Functional Requirements

- Upload and chunk multiple document types (PDF, TXT, CSV, DOCX)
- Generate embeddings and store them in a vector database
- Deduplicate content before indexing
- Perform semantic similarity search
- Generate summaries using an LLM
- Evaluate summaries using ROUGE scores
- Persist indexed data across sessions

---

## ğŸ› ï¸ Tools & Technologies Used

| Category | Tools |
|------|------|
| Frontend | Streamlit |
| Vector Database | Pinecone |
| Embeddings | Local / Free Embedding Model |
| LLM | OpenAI-compatible LLM (via Agent abstraction) |
| Chunking | Custom text chunker |
| Evaluation | ROUGE |
| Hashing | SHA-256 |
| Environment | Python, dotenv |

---

## ğŸ§  System Architecture & Approach

### 1ï¸âƒ£ Document Ingestion
- Documents are uploaded via Streamlit UI
- Each document is split into overlapping chunks
- Each chunk is hashed using **SHA-256** to create a stable ID

### 2ï¸âƒ£ Deduplicated Indexing
- Before indexing, Pinecone is checked for existing chunk IDs
- Duplicate chunks are skipped automatically
- New chunks are embedded and upserted in batches

### 3ï¸âƒ£ Semantic Retrieval
- User query is embedded
- Top-K similar chunks are retrieved from Pinecone
- Retrieval works even if indexing is not done in the current session

### 4ï¸âƒ£ Summarization
- Retrieved chunks are normalized safely
- Combined context is sent to the LLM
- Summary length is controlled by user input

### 5ï¸âƒ£ Evaluation
- Generated summaries are evaluated using ROUGE metrics
- Manual human evaluation guidelines are also provided

---

## âœ¨ Key Features

- âœ… Persistent semantic search (session-independent)
- âœ… SHA-256 based deduplication
- âœ… Batched Pinecone upserts (safe for size limits)
- âœ… Robust chunk normalization (prevents NoneType errors)
- âœ… Explicit user-triggered summarization
- âœ… Automatic + manual evaluation support
- âœ… Clean, modular, production-ready codebase
- âœ… Document-level filtering in retrieval

---

## ğŸ§¾ Project Structure

```
RAG/
â”‚
â”œâ”€â”€ app.py # Streamlit application entry point
â”œâ”€â”€ reference_summaries.py # reference text for evaluation
â”‚
â”œâ”€â”€ vectorstore/
â”‚ â”œâ”€â”€ indexer.py # Chunk embedding & Pinecone indexing
â”‚ â”œâ”€â”€ retriever.py # Semantic retrieval logic
â”‚ â”œâ”€â”€ embeddings.py # Embedding generation (local/free)
â”‚ â””â”€â”€ pinecone_client.py # Pinecone index existence checks
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ file_loader.py # File parsing & chunking
â”‚ â””â”€â”€ hashing.py # SHA-256 content hashing
â”‚
â”œâ”€â”€ crew/
â”‚ â””â”€â”€ rag_crew.py # LLM / Agent abstraction
â”‚
â”œâ”€â”€ evaluation/
â”‚ â””â”€â”€ rouge_eval.py # ROUGE evaluation logic
â”‚
â”œâ”€â”€ .env # Environment variables
â”œâ”€â”€ requirements.txt # Project dependencies
â””â”€â”€ README.md # Project documentation
```
## âš™ï¸ Environment Variables
```
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX=your_index_name
```

## â–¶ï¸ How to Run the Project
```
pip install -r requirements.txt
streamlit run app.py
```


---

## ğŸ§ª Evaluation Metrics

- ROUGE-1
- ROUGE-2
- ROUGE-L
- Human evaluation:
  - Relevance
  - Coverage
  - Coherence
  - Correctness

---

## ğŸ§  Design Decisions (Why This Approach)

- **Vector DB (Pinecone)**: Enables scalable semantic search
- **Hash-based IDs**: Prevents duplicate embeddings
- **Chunking with overlap**: Preserves semantic continuity
- **Session-independent retrieval**: Aligns with real-world RAG systems
- **Modular architecture**: Easy to extend or replace components

---

## ğŸš€ Future Enhancements

- Multi-document comparative summaries
- Search accuracy metrics (Precision@K, Recall@K)
- Feedback-based re-ranking
- UI-based evaluation dashboards

---

## ğŸ“Œ Use Cases

- Book summarization
- Research assistance
- Knowledge base search
- Academic and enterprise document analysis
- Interview-ready RAG project demonstration

---

## ğŸ‘©â€ğŸ’» Author

**Mounica Srinivasan**  
| Aspiring Data Scientist  
RAG â€¢ NLP â€¢ Vector Databases â€¢ LLM Applications


