# ğŸ“˜ RAG-based Document Search, Summarization & Evaluation System
---
Streamlit APP link:

---
A **production-style Retrieval-Augmented Generation (RAG) application** built with **Streamlit**, **Pinecone**, **LangChain**, and **OpenAI**, designed to ingest documents, perform **hybrid retrieval**, generate **ChatGPT-like answers**, and support both **automatic and human evaluation**.

This system supports **multi-topic content ingestion** and is not limited to a single domain.

---

## ğŸš€ Key Highlights

- Multi-format document ingestion (PDF, DOCX, TXT, CSV)
- Full-document deduplication using content hashing
- Persistent document registry across sessions
- Vector-based semantic retrieval (Pinecone)
- Hybrid retrieval (Vector + BM25-style keyword matching)
- ChatGPT-style answer generation (answer-focused, not generic summaries)
- Question-intent-aware responses (advantages, disadvantages, steps, comparisons)
- ROUGE-based automatic evaluation
- Industry-style human evaluation stored in CSV
- CrewAI is **NOT used** (custom LangChain orchestration instead)

---

## ğŸ§  Architecture Overview

User Query  
â†“  
Hybrid Retrieval (Vector Similarity + Keyword Matching)  
â†“  
Context Assembly  
â†“  
LLM Answer Generation (ChatGPT-style)  
â†“  
Evaluation (ROUGE + Human Review)

---

## ğŸ—‚ï¸ Project Structure

```# ğŸ“˜ RAG-based Document Search, Summarization & Evaluation System

A **production-style Retrieval-Augmented Generation (RAG) application** built with **Streamlit**, **Pinecone**, **LangChain**, and **OpenAI**, designed to ingest documents, perform **hybrid retrieval**, generate **ChatGPT-like answers**, and support both **automatic and human evaluation**.

This system supports **multi-topic content ingestion** and is not limited to a single domain.

---

## ğŸš€ Key Highlights

- Multi-format document ingestion (PDF, DOCX, TXT, CSV)
- Full-document deduplication using content hashing
- Persistent document registry across sessions
- Vector-based semantic retrieval (Pinecone)
- Hybrid retrieval (Vector + BM25-style keyword matching)
- ChatGPT-style answer generation (answer-focused, not generic summaries)
- Question-intent-aware responses (advantages, disadvantages, steps, comparisons)
- ROUGE-based automatic evaluation
- Industry-style human evaluation stored in CSV
- CrewAI is **NOT used** (custom LangChain orchestration instead)

---

## ğŸ§  Architecture Overview

User Query  
â†“  
Hybrid Retrieval (Vector Similarity + Keyword Matching)  
â†“  
Context Assembly  
â†“  
LLM Answer Generation (ChatGPT-style)  
â†“  
Evaluation (ROUGE + Human Review)

---

## ğŸ—‚ï¸ Project Structure

```
RAG/
â”‚
â”œâ”€â”€ app.py # Streamlit application (UI + orchestration)
â”‚
â”œâ”€â”€ docs_loader.py # Persistent document registry (CSV-based)
â”‚
â”œâ”€â”€ reference_summaries.py # Gold reference answers for evaluation
â”‚
â”œâ”€â”€ vectorstore/
â”‚ â”œâ”€â”€ embeddings.py # Embedding generation (OpenAI)
â”‚ â”œâ”€â”€ indexer.py # Chunk upsert + deduplication (Pinecone)
â”‚ â””â”€â”€ retriever.py # Hybrid retrieval logic
â”‚
â”œâ”€â”€ crew/
â”‚ â””â”€â”€ rag_crew.py # Prompt-engineered RAG answer generation
â”‚
â”œâ”€â”€ evaluation/
â”‚ â””â”€â”€ rouge_eval.py # ROUGE score evaluation
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ file_loader.py # Document parsing & chunking
â”‚ â””â”€â”€ hashing.py # Content hash for deduplication
â”‚
â”œâ”€â”€ indexed_documents.csv # Persistent indexed document registry
â”œâ”€â”€ human_evaluations.csv # Stored human evaluation results
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```


---

## ğŸ“š Features in Detail

### 1ï¸âƒ£ Document Indexing
- Upload one or more documents
- Chunking with overlap for context preservation
- Batched embedding generation
- Safe Pinecone upserts (batch size controlled)
- Full-document deduplication using content hash
- Persistent document registry stored in CSV

---

### 2ï¸âƒ£ Persistent Document Selection
- Previously indexed documents are available across sessions
- Dropdown shows **document names**, not hashes
- Supports:
  - Single-document querying
  - Cross-document querying (â€œAll Documentsâ€)

---

### 3ï¸âƒ£ Hybrid Retrieval (Vector + Keyword)
- Semantic similarity via embeddings
- Keyword relevance via BM25-style matching
- Improves factual grounding and intent alignment
- Reduces irrelevant chunk retrieval

---

### 4ï¸âƒ£ High-Quality Answer Generation
- ChatGPT-style responses
- Answer-focused (not summary-heavy)
- Uses only retrieved context
- Query-intent aware:
  - Advantages / Disadvantages
  - Steps / Processes
  - Comparisons
  - Direct factual answers
- Clean, structured output using bullets or short paragraphs

---

### 5ï¸âƒ£ Evaluation Layer

#### ğŸ”¹ Automatic Evaluation
- ROUGE-1, ROUGE-2, ROUGE-L
- Compared against gold reference answers

#### ğŸ”¹ Human Evaluation (Industry Style)
- Ratings for:
  - Relevance
  - Coverage
  - Correctness
  - Faithfulness (hallucination check)
  - Coherence
- Evaluator notes
- Stored persistently in CSV with timestamps

---

## ğŸ› ï¸ Tech Stack

- UI: Streamlit
- LLM: OpenAI (via LangChain)
- Embeddings: OpenAI
- Vector Database: Pinecone
- Retrieval: Hybrid (Vector + BM25-style)
- Evaluation: ROUGE + Human Review
- Persistence: CSV-based registry

---

## âŒ What This Project Does NOT Use

- CrewAI (folder name retained, but orchestration is custom)
- External managed RAG frameworks
- Session-only document tracking

This ensures **full transparency and engineering control**.

---

## ğŸ“ˆ Use Cases

- Internal knowledge base Q&A
- Learning assistant across mixed topics
- RAG system prototyping
- Interview-ready GenAI project
- Evaluation framework experimentation

---

## ğŸ”® Future Enhancements

- Chunk-level citation highlighting
- Retrieval confidence scoring
- Cross-encoder re-ranking
- Follow-up question memory
- Adaptive top-k retrieval

---

## ğŸ‘¤ Author Notes

This project was built with emphasis on:
- Correct RAG principles
- Industry-aligned evaluation
- Debuggability and clarity


It demonstrates **practical GenAI engineering**, not just API usage.
